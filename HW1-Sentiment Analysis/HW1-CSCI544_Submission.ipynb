{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/shreenidhihegde/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/shreenidhihegde/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import contractions\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install bs4 # in case you don't have it installed\n",
    "\n",
    "# Dataset: https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Kitchen_v1_00.tsv.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 16148: expected 15 fields, saw 22\\nSkipping line 20100: expected 15 fields, saw 22\\nSkipping line 45178: expected 15 fields, saw 22\\nSkipping line 48700: expected 15 fields, saw 22\\nSkipping line 63331: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 86053: expected 15 fields, saw 22\\nSkipping line 88858: expected 15 fields, saw 22\\nSkipping line 115017: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 137366: expected 15 fields, saw 22\\nSkipping line 139110: expected 15 fields, saw 22\\nSkipping line 165540: expected 15 fields, saw 22\\nSkipping line 171813: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 203723: expected 15 fields, saw 22\\nSkipping line 209366: expected 15 fields, saw 22\\nSkipping line 211310: expected 15 fields, saw 22\\nSkipping line 246351: expected 15 fields, saw 22\\nSkipping line 252364: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 267003: expected 15 fields, saw 22\\nSkipping line 268957: expected 15 fields, saw 22\\nSkipping line 303336: expected 15 fields, saw 22\\nSkipping line 306021: expected 15 fields, saw 22\\nSkipping line 311569: expected 15 fields, saw 22\\nSkipping line 316767: expected 15 fields, saw 22\\nSkipping line 324009: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 359107: expected 15 fields, saw 22\\nSkipping line 368367: expected 15 fields, saw 22\\nSkipping line 381180: expected 15 fields, saw 22\\nSkipping line 390453: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 412243: expected 15 fields, saw 22\\nSkipping line 419342: expected 15 fields, saw 22\\nSkipping line 457388: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 459935: expected 15 fields, saw 22\\nSkipping line 460167: expected 15 fields, saw 22\\nSkipping line 466460: expected 15 fields, saw 22\\nSkipping line 500314: expected 15 fields, saw 22\\nSkipping line 500339: expected 15 fields, saw 22\\nSkipping line 505396: expected 15 fields, saw 22\\nSkipping line 507760: expected 15 fields, saw 22\\nSkipping line 513626: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 527638: expected 15 fields, saw 22\\nSkipping line 534209: expected 15 fields, saw 22\\nSkipping line 535687: expected 15 fields, saw 22\\nSkipping line 547671: expected 15 fields, saw 22\\nSkipping line 549054: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 599929: expected 15 fields, saw 22\\nSkipping line 604776: expected 15 fields, saw 22\\nSkipping line 609937: expected 15 fields, saw 22\\nSkipping line 632059: expected 15 fields, saw 22\\nSkipping line 638546: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 665017: expected 15 fields, saw 22\\nSkipping line 677680: expected 15 fields, saw 22\\nSkipping line 684370: expected 15 fields, saw 22\\nSkipping line 720217: expected 15 fields, saw 29\\n'\n",
      "b'Skipping line 723240: expected 15 fields, saw 22\\nSkipping line 723433: expected 15 fields, saw 22\\nSkipping line 763891: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 800288: expected 15 fields, saw 22\\nSkipping line 802942: expected 15 fields, saw 22\\nSkipping line 803379: expected 15 fields, saw 22\\nSkipping line 805122: expected 15 fields, saw 22\\nSkipping line 821899: expected 15 fields, saw 22\\nSkipping line 831707: expected 15 fields, saw 22\\nSkipping line 842829: expected 15 fields, saw 22\\nSkipping line 843604: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 863904: expected 15 fields, saw 22\\nSkipping line 875655: expected 15 fields, saw 22\\nSkipping line 886796: expected 15 fields, saw 22\\nSkipping line 892299: expected 15 fields, saw 22\\nSkipping line 902518: expected 15 fields, saw 22\\nSkipping line 903079: expected 15 fields, saw 22\\nSkipping line 912678: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 932953: expected 15 fields, saw 22\\nSkipping line 936838: expected 15 fields, saw 22\\nSkipping line 937177: expected 15 fields, saw 22\\nSkipping line 947695: expected 15 fields, saw 22\\nSkipping line 960713: expected 15 fields, saw 22\\nSkipping line 965225: expected 15 fields, saw 22\\nSkipping line 980776: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 999318: expected 15 fields, saw 22\\nSkipping line 1007247: expected 15 fields, saw 22\\nSkipping line 1015987: expected 15 fields, saw 22\\nSkipping line 1018984: expected 15 fields, saw 22\\nSkipping line 1028671: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 1063360: expected 15 fields, saw 22\\nSkipping line 1066195: expected 15 fields, saw 22\\nSkipping line 1066578: expected 15 fields, saw 22\\nSkipping line 1066869: expected 15 fields, saw 22\\nSkipping line 1068809: expected 15 fields, saw 22\\nSkipping line 1069505: expected 15 fields, saw 22\\nSkipping line 1087983: expected 15 fields, saw 22\\nSkipping line 1108184: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 1118137: expected 15 fields, saw 22\\nSkipping line 1142723: expected 15 fields, saw 22\\nSkipping line 1152492: expected 15 fields, saw 22\\nSkipping line 1156947: expected 15 fields, saw 22\\nSkipping line 1172563: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 1209254: expected 15 fields, saw 22\\nSkipping line 1212966: expected 15 fields, saw 22\\nSkipping line 1236533: expected 15 fields, saw 22\\nSkipping line 1237598: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 1273825: expected 15 fields, saw 22\\nSkipping line 1277898: expected 15 fields, saw 22\\nSkipping line 1283654: expected 15 fields, saw 22\\nSkipping line 1286023: expected 15 fields, saw 22\\nSkipping line 1302038: expected 15 fields, saw 22\\nSkipping line 1305179: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 1326022: expected 15 fields, saw 22\\nSkipping line 1338120: expected 15 fields, saw 22\\nSkipping line 1338503: expected 15 fields, saw 22\\nSkipping line 1338849: expected 15 fields, saw 22\\nSkipping line 1341513: expected 15 fields, saw 22\\nSkipping line 1346493: expected 15 fields, saw 22\\nSkipping line 1373127: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 1389508: expected 15 fields, saw 22\\nSkipping line 1413951: expected 15 fields, saw 22\\nSkipping line 1433626: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 1442698: expected 15 fields, saw 22\\nSkipping line 1472982: expected 15 fields, saw 22\\nSkipping line 1482282: expected 15 fields, saw 22\\nSkipping line 1487808: expected 15 fields, saw 22\\nSkipping line 1500636: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 1511479: expected 15 fields, saw 22\\nSkipping line 1532302: expected 15 fields, saw 22\\nSkipping line 1537952: expected 15 fields, saw 22\\nSkipping line 1539951: expected 15 fields, saw 22\\nSkipping line 1541020: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 1594217: expected 15 fields, saw 22\\nSkipping line 1612264: expected 15 fields, saw 22\\nSkipping line 1615907: expected 15 fields, saw 22\\nSkipping line 1621859: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 1653542: expected 15 fields, saw 22\\nSkipping line 1671537: expected 15 fields, saw 22\\nSkipping line 1672879: expected 15 fields, saw 22\\nSkipping line 1674523: expected 15 fields, saw 22\\nSkipping line 1677355: expected 15 fields, saw 22\\nSkipping line 1703907: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 1713046: expected 15 fields, saw 22\\nSkipping line 1722982: expected 15 fields, saw 22\\nSkipping line 1727290: expected 15 fields, saw 22\\nSkipping line 1744482: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 1803858: expected 15 fields, saw 22\\nSkipping line 1810069: expected 15 fields, saw 22\\nSkipping line 1829751: expected 15 fields, saw 22\\nSkipping line 1831699: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 1863131: expected 15 fields, saw 22\\nSkipping line 1867917: expected 15 fields, saw 22\\nSkipping line 1874790: expected 15 fields, saw 22\\nSkipping line 1879952: expected 15 fields, saw 22\\nSkipping line 1880501: expected 15 fields, saw 22\\nSkipping line 1886655: expected 15 fields, saw 22\\nSkipping line 1887888: expected 15 fields, saw 22\\nSkipping line 1894286: expected 15 fields, saw 22\\nSkipping line 1895400: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 1904040: expected 15 fields, saw 22\\nSkipping line 1907604: expected 15 fields, saw 22\\nSkipping line 1915739: expected 15 fields, saw 22\\nSkipping line 1921514: expected 15 fields, saw 22\\nSkipping line 1939428: expected 15 fields, saw 22\\nSkipping line 1944342: expected 15 fields, saw 22\\nSkipping line 1949699: expected 15 fields, saw 22\\nSkipping line 1961872: expected 15 fields, saw 22\\n'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 1968846: expected 15 fields, saw 22\\nSkipping line 1999941: expected 15 fields, saw 22\\nSkipping line 2001492: expected 15 fields, saw 22\\nSkipping line 2011204: expected 15 fields, saw 22\\nSkipping line 2025295: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 2041266: expected 15 fields, saw 22\\nSkipping line 2073314: expected 15 fields, saw 22\\nSkipping line 2080133: expected 15 fields, saw 22\\nSkipping line 2088521: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 2103490: expected 15 fields, saw 22\\nSkipping line 2115278: expected 15 fields, saw 22\\nSkipping line 2153174: expected 15 fields, saw 22\\nSkipping line 2161731: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 2165250: expected 15 fields, saw 22\\nSkipping line 2175132: expected 15 fields, saw 22\\nSkipping line 2206817: expected 15 fields, saw 22\\nSkipping line 2215848: expected 15 fields, saw 22\\nSkipping line 2223811: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 2257265: expected 15 fields, saw 22\\nSkipping line 2259163: expected 15 fields, saw 22\\nSkipping line 2263291: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 2301943: expected 15 fields, saw 22\\nSkipping line 2304371: expected 15 fields, saw 22\\nSkipping line 2306015: expected 15 fields, saw 22\\nSkipping line 2312186: expected 15 fields, saw 22\\nSkipping line 2314740: expected 15 fields, saw 22\\nSkipping line 2317754: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 2383514: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 2449763: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 2589323: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 2775036: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 2935174: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 3078830: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 3123091: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 3185533: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 4150395: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 4748401: expected 15 fields, saw 22\\n'\n"
     ]
    }
   ],
   "source": [
    " \"\"\"\n",
    " Here we are reading the data from tsv file as pandas dataframe.\n",
    " We use '\\t' to seperate columns by a tab and \"error_bad_lines = False\" to drop bad lines from the DataFrame\n",
    " \n",
    " \"\"\"  \n",
    "text_data = pd.read_csv(\"data.tsv\",error_bad_lines = False, sep = '\\t')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keep Reviews and Ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " -x--x- Three sample reviews with corresponding ratings -x--x- \n",
      "\n",
      "         star_rating                                        review_body\n",
      "2264076          5.0  Do you know what's better than a Seattle Seaha...\n",
      "2973343          4.0  the soup bowls are little bit smaller for nood...\n",
      "2090625          3.0   They stain easy & handles get really hot to grab\n",
      "\n",
      " -x--x- Statistics of the ratings -x--x- \n",
      "\n",
      "5.0    3124595\n",
      "4.0     731701\n",
      "1.0     426870\n",
      "3.0     349539\n",
      "2.0     241939\n",
      "Name: star_rating, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "    #We are using dropna to look for missing values in the rows which has no review body and drop the corresponding rows.\n",
    "    \n",
    "    text_data.dropna(subset = ['review_body'], inplace= True)\n",
    "    text_data.dropna(subset = ['star_rating'], inplace= True)\n",
    "    \n",
    "    \n",
    "    # We Keep only the reviews and ratings of the initial data\n",
    "    text_data = text_data[[\"star_rating\",\"review_body\"]]\n",
    "    \n",
    "    # We are including 3 sample reviews with the corresponding rating\n",
    "    print (\"\\n -x--x- Three sample reviews with corresponding ratings -x--x- \\n\")\n",
    "    print (text_data.sample(n=3, random_state=100))\n",
    "    \n",
    "    # We are reporting statistics of the ratings\n",
    "    star_counts = text_data[\"star_rating\"].value_counts()\n",
    "    print(\"\\n -x--x- Statistics of the ratings -x--x- \\n\")\n",
    "    print(star_counts)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Labelling Reviews:\n",
    "## The reviews with rating 4,5 are labelled to be 1 and 1,2 are labelled as 0. Discard the reviews with rating 3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " -x--x- Counts of all review labels before removing the reviews with rating 3 -x--x-\n",
      "\n",
      " 1    3856296\n",
      " 0     668809\n",
      "-1     349539\n",
      "Name: label, dtype: int64\n",
      "\n",
      " -x--x- Counts of all review labels after removing the reviews with rating 3 -x--x- \n",
      "\n",
      "1    3856296\n",
      "0     668809\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Using np.where we are putting conditions to label the ratings >=4 as 1 and ratins <=2 as 0. \n",
    "#The remainign ratings which is labelled as 3 will be labelled as -1.\n",
    "\n",
    "text_data['label'] = np.where(text_data[\"star_rating\"] >=4,1, np.where(text_data[\"star_rating\"] <= 2,0,-1))\n",
    "rating_count = text_data['label'].value_counts()\n",
    "\n",
    "#We are getting the counts of all review labels before removing the reviews with rating 3\n",
    "print(\"\\n -x--x- Counts of all review labels before removing the reviews with rating 3 -x--x-\\n\")\n",
    "print(rating_count)\n",
    "\n",
    "#Discarding the reviews with rating 3\n",
    "text_data = text_data[text_data[\"label\"]!= -1]\n",
    "\n",
    "#We are getting the counts of all review labels after removing the reviews with rating 3\n",
    "print(\"\\n -x--x- Counts of all review labels after removing the reviews with rating 3 -x--x- \\n\")\n",
    "print(text_data['label'].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## We select 200000 reviews randomly with 100,000 positive and 100,000 negative reviews.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " -x--x- The average length of the reviews in terms of character length in the dataset before cleaning is -x--x- \n",
      "321.69\n",
      "\n",
      " -x--x- The sample reviews before cleaning -x--x- \n",
      "\n",
      "4466764    I was concerned about BPA and knew that using ...\n",
      "230597     These koozies are great!  My boyfriend and I u...\n",
      "1984586    Beautiful engineering but you spend the rest o...\n",
      "Name: review_body, dtype: object\n"
     ]
    }
   ],
   "source": [
    "#We select sample of 100,000 positive and 100,000 negative reviews\n",
    "negative = text_data.label[text_data.label.eq(0)].sample(100000).index\n",
    "positive = text_data.label[text_data.label.eq(1)].sample(100000).index\n",
    "\n",
    "#We combine both the selected samples\n",
    "text_data = text_data.loc[negative.union(positive)]\n",
    "\n",
    "# We are getting mean of each row based on characters in each row and then finding the mean of all the rows\n",
    "review_mean = text_data.review_body.apply(lambda x : len(str(x))).mean()\n",
    "# print 3 sample reviews\n",
    "\n",
    "print(f'\\n -x--x- The average length of the reviews in terms of character length in the dataset before cleaning is -x--x- \\n{review_mean:.2f}')\n",
    "print(\"\\n -x--x- The sample reviews before cleaning -x--x- \\n\")\n",
    "print(text_data[\"review_body\"].sample(n=3, random_state=100))\n",
    "\n",
    "\n",
    "                                          \n",
    "                                                                                                                 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning\n",
    "\n",
    "## Convert the all reviews into the lower case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We use str.lower() to convert all the characters into lower characters\n",
    "text_data[\"review_body\"] = text_data[\"review_body\"].str.lower()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Remove the HTML and URLs from the reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We use Beatiful soup to remove all the HTML Tags from the dataframe\n",
    "text_data[\"review_body\"] = text_data[\"review_body\"].apply(lambda x: BeautifulSoup(str(x),\"html.parser\").get_text())\n",
    "\n",
    "# We are here removing the URLs from the reviews\n",
    "Url_pattern = r'\\s*(https?://|www\\.)+\\S+(\\s+|$)'\n",
    "text_data[\"review_body\"] = text_data[\"review_body\"].apply(lambda x: re.sub(Url_pattern, \" \", str(x), flags=re.UNICODE))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## remove non-alphabetical characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First we remove all the words starts with digits\n",
    "text_data[\"review_body\"] = text_data[\"review_body\"].apply(lambda x: re.sub(r\"[^\\D']+\", \" \", str(x), flags=re.UNICODE))\n",
    "\n",
    "#Next we remove all the words which starts with non alphabetic characters\n",
    "text_data[\"review_body\"] = text_data[\"review_body\"].apply(lambda x: re.sub(r\"[^\\w']+\", \" \", str(x), flags=re.UNICODE))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove the extra spaces between the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We remove the extra spaces from the dataset\n",
    "# text_data[\"review_body\"] = text_data[\"review_body\"].replace('\\s+', ' ', regex=True)\n",
    "text_data[\"review_body\"] = text_data[\"review_body\"].apply(lambda x: re.sub(r\"\\s+\", \" \", str(x), flags=re.UNICODE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## perform contractions on the reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We perform contractions using contractions.fix\n",
    "\n",
    "def contractionfunction(i):\n",
    "    i = i.apply(lambda x: contractions.fix(x))\n",
    "    return i\n",
    "\n",
    "text_data[\"review_body\"] = contractionfunction(text_data[\"review_body\"])\n",
    "#We convert the characters into lowercase again as after contractions some words will get capitalized Ex: \"i'm will\" become \"I am\" after contraction.\n",
    "text_data[\"review_body\"] = text_data[\"review_body\"].str.lower()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " -x--x- The average length of the reviews in terms of character length in the dataset after cleaning is -x--x-\n",
      " 308.509475\n",
      "\n",
      " -x--x- The sample reviews after cleaning -x--x-\n",
      "\n",
      "4466764    i was concerned about bpa and knew that using ...\n",
      "230597     these koozies are great my boyfriend and i use...\n",
      "1984586    beautiful engineering but you spend the rest o...\n",
      "Name: review_body, dtype: object\n"
     ]
    }
   ],
   "source": [
    "review_mean_new = text_data.review_body.apply(lambda x :len(str(x))).mean()\n",
    "print(\"\\n -x--x- The average length of the reviews in terms of character length in the dataset after cleaning is -x--x-\\n\", review_mean_new)\n",
    "print(\"\\n -x--x- The sample reviews after cleaning -x--x-\\n\")\n",
    "print(text_data[\"review_body\"].sample(n=3, random_state=100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## remove the stop words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are removing stop words \n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "text_data[\"review_body\"] = text_data[\"review_body\"].apply(lambda x: \" \".join([i for i in x.split() if i not in stop_words]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## perform lemmatization  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We perform lemmatization on the data\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "text_data[\"review_body\"] = text_data[\"review_body\"].apply(lambda x: \" \".join(lemmatizer.lemmatize(i) for i in x.split()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " -x--x- The average length of the reviews in terms of character length in the dataset after pre-processing is -x--x- \n",
      " 188.91869\n",
      "\n",
      " -x--x- The sample reviews after pre-processing -x--x-\n",
      "\n",
      "4466764    concerned bpa knew using reusing throw away bo...\n",
      "230597     koozies great boyfriend used right getting rec...\n",
      "1984586    beautiful engineering spend rest money buying ...\n",
      "Name: review_body, dtype: object\n"
     ]
    }
   ],
   "source": [
    "review_mean_3 = text_data.review_body.apply(lambda x : np.mean(len(str(x)))).mean()\n",
    "print(\"\\n -x--x- The average length of the reviews in terms of character length in the dataset after pre-processing is -x--x- \\n\", review_mean_3)\n",
    "print(\"\\n -x--x- The sample reviews after pre-processing -x--x-\\n\")\n",
    "print(text_data[\"review_body\"].sample(n=3, random_state=100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(160000,)\n"
     ]
    }
   ],
   "source": [
    "# We use train_test_split from sklearn to split the data into 80% training and 20% testing sets\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(text_data[\"review_body\"], text_data[\"label\"], test_size=0.2, random_state=100)\n",
    "\n",
    "#We ignore terms that have a document frequency strictly higher 0.7 and document frequency strictly lower than 1.\n",
    "print(X_train.shape)\n",
    "\n",
    "#vectorizer = TfidfVectorizer(min_df=1, max_df=0.7)\n",
    "#Xtrain = vectorizer.fit_transform(X_train)\n",
    "#Xtest = vectorizer.transform(X_test)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 50261)\t0.06901096342220804\n",
      "  (0, 30367)\t0.08581894523851961\n",
      "  (0, 22092)\t0.0572202415310822\n",
      "  (0, 34060)\t0.08789934045866332\n",
      "  (0, 22266)\t0.07387964188380705\n",
      "  (0, 43499)\t0.07670364997693523\n",
      "  (0, 32171)\t0.0342705606317413\n",
      "  (0, 1928)\t0.054736257249859206\n",
      "  (0, 6263)\t0.06485972126988566\n",
      "  (0, 23208)\t0.09838716528339621\n",
      "  (0, 19726)\t0.07716992316531952\n",
      "  (0, 26331)\t0.07166448130414782\n",
      "  (0, 42784)\t0.08066419777412423\n",
      "  (0, 37153)\t0.06727238461201715\n",
      "  (0, 20162)\t0.09569947342605921\n",
      "  (0, 14720)\t0.07169203441194372\n",
      "  (0, 30269)\t0.04740790084487304\n",
      "  (0, 32866)\t0.06724831097923202\n",
      "  (0, 22352)\t0.05553387400441643\n",
      "  (0, 36681)\t0.09931826965661106\n",
      "  (0, 17250)\t0.07273399589876606\n",
      "  (0, 31052)\t0.05059779551756077\n",
      "  (0, 21115)\t0.0555494367244654\n",
      "  (0, 42204)\t0.060463217989987404\n",
      "  (0, 29455)\t0.06426478989777605\n",
      "  :\t:\n",
      "  (0, 8681)\t0.1473710759325334\n",
      "  (0, 38836)\t0.09073054412759769\n",
      "  (0, 32088)\t0.0850062562639632\n",
      "  (0, 7182)\t0.513316707100233\n",
      "  (0, 2481)\t0.061211869802416316\n",
      "  (0, 45977)\t0.13165992725561584\n",
      "  (0, 30639)\t0.10323024026210798\n",
      "  (0, 33405)\t0.07310753626700037\n",
      "  (0, 32262)\t0.08081208697312743\n",
      "  (0, 32038)\t0.07335396005973949\n",
      "  (0, 44106)\t0.08609556190365693\n",
      "  (0, 27125)\t0.0500230482839046\n",
      "  (0, 50281)\t0.03573069803788263\n",
      "  (0, 33321)\t0.28487336680768666\n",
      "  (0, 16611)\t0.07601721370311841\n",
      "  (0, 17709)\t0.04746877207490925\n",
      "  (0, 42413)\t0.23038363630284503\n",
      "  (0, 28978)\t0.08813094343307057\n",
      "  (0, 16960)\t0.12871795931992416\n",
      "  (0, 50290)\t0.04405519284434314\n",
      "  (0, 16176)\t0.06176582492974976\n",
      "  (0, 4285)\t0.05917301539432548\n",
      "  (0, 17650)\t0.07797774353453331\n",
      "  (0, 44737)\t0.12401715914959983\n",
      "  (0, 31257)\t0.13770524409461968\n"
     ]
    }
   ],
   "source": [
    "print(Xtrain[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we standardize the data using StandardScaler from sklearn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#Create the instance\n",
    "sc = StandardScaler(with_mean=False)\n",
    "\n",
    "#We fit the scaler to the training feauture set only\n",
    "sc.fit(Xtrain)\n",
    "\n",
    "#Scale or Transform the training and the testing tests using the scaler that was fitted to training data\n",
    "Xtrain_std = sc.transform(Xtrain)\n",
    "Xtest_std = sc.transform(Xtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " -x--x- Accuracy, Precision, Recall, and f1-score on test data -x--x-\n",
      "\n",
      "Testing Accuracy:0.8279\n",
      "Testing f1_Score:0.8274\n",
      "Testing Precision:0.8267\n",
      "Testing recall_score:0.8280\n",
      "\n",
      " -x--x- Accuracy, Precision, Recall, and f1-score on train data -x--x-\n",
      "\n",
      "Training Accuracy:0.9262\n",
      "Training f1_Score:0.9263\n",
      "Training Precision:0.9266\n",
      "Training recall_score:0.9259\n"
     ]
    }
   ],
   "source": [
    "#implementation of perceptron\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "# Create a perceptron object with the parameters: 40 iterations (epochs) over the data, and a learning rate of 0.1\n",
    "ppn = Perceptron(max_iter=40, eta0=0.1, random_state=100)\n",
    "\n",
    "# Fit the model to the standardized data\n",
    "ppn.fit(Xtrain_std, y_train)\n",
    "\n",
    "\n",
    "# Apply the trained perceptron on the X data to make predicts for the Y test data\n",
    "y_pred_test = ppn.predict(Xtest_std)\n",
    "\n",
    "#We measure the performance using the \"accuracy_score,f1_score,precision_score and recall_score\"\n",
    "\n",
    "print(\"\\n -x--x- Accuracy, Precision, Recall, and f1-score on test data -x--x-\\n\")\n",
    "\n",
    "print(f'Testing Accuracy:{accuracy_score(y_test, y_pred_test):.4f}')\n",
    "print(f'Testing f1_Score:{f1_score(y_test, y_pred_test):.4f}')\n",
    "print(f'Testing Precision:{precision_score(y_test, y_pred_test):.4f}')\n",
    "print(f'Testing recall_score:{recall_score(y_test, y_pred_test):.4f}')\n",
    "\n",
    "#Apply the trained perceptron on the data to make predicts for the trained data\n",
    "y_pred_tarin = ppn.predict(Xtrain_std)\n",
    "\n",
    "\n",
    "print(\"\\n -x--x- Accuracy, Precision, Recall, and f1-score on train data -x--x-\\n\")\n",
    "\n",
    "print(f'Training Accuracy:{accuracy_score(y_train, y_pred_tarin):.4f}')\n",
    "print(f'Training f1_Score:{f1_score(y_train, y_pred_tarin):.4f}')\n",
    "print(f'Training Precision:{precision_score(y_train, y_pred_tarin):.4f}')\n",
    "print(f'Training recall_score:{recall_score(y_train, y_pred_tarin):.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " -x--x- Accuracy, Precision, Recall, and f1-score on test data -x--x- \n",
      "\n",
      "Testing Accuracy:0.8977\n",
      "Testing f1_Score:0.8970\n",
      "Testing Precision:0.8992\n",
      "Testing recall_score:0.8949\n",
      "\n",
      " -x--x- Accuracy, Precision, Recall, and f1-score on train data -x--x- \n",
      "\n",
      "Training Accuracy:0.9335\n",
      "Training f1_Score:0.9334\n",
      "Training Precision:0.9353\n",
      "Training recall_score:0.9315\n"
     ]
    }
   ],
   "source": [
    "#implementation of SVM\n",
    "from sklearn import svm\n",
    "\n",
    "#Create a Classifier for svm\n",
    "clf = svm.LinearSVC() # We are using Linear Kernel\n",
    "\n",
    "#Train the model using the training sets\n",
    "clf.fit(Xtrain, y_train)\n",
    "\n",
    "#Apply the trained svm on Xtrain_std data to make predictions for the test data\n",
    "y_pred_test = clf.predict(Xtest)\n",
    "\n",
    "\n",
    "print(\"\\n -x--x- Accuracy, Precision, Recall, and f1-score on test data -x--x- \\n\")\n",
    "\n",
    "print(f'Testing Accuracy:{accuracy_score(y_test, y_pred_test):.4f}')\n",
    "print(f'Testing f1_Score:{f1_score(y_test, y_pred_test):.4f}')\n",
    "print(f'Testing Precision:{precision_score(y_test, y_pred_test):.4f}')\n",
    "print(f'Testing recall_score:{recall_score(y_test, y_pred_test):.4f}')\n",
    "\n",
    "#Apply the trained perceptron on the data to make predicts for the trained data\n",
    "y_pred_tarin = clf.predict(Xtrain)\n",
    "\n",
    "#We measure the performance using the \"accuracy_score,f1_Score,Precision_score and recall_score\"\n",
    "\n",
    "print(\"\\n -x--x- Accuracy, Precision, Recall, and f1-score on train data -x--x- \\n\")\n",
    "\n",
    "print(f'Training Accuracy:{accuracy_score(y_train, y_pred_tarin):.4f}')\n",
    "print(f'Training f1_Score:{f1_score(y_train, y_pred_tarin):.4f}')\n",
    "print(f'Training Precision:{precision_score(y_train, y_pred_tarin):.4f}')\n",
    "print(f'Training recall_score:{recall_score(y_train, y_pred_tarin):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-x--x- Accuracy, Precision, Recall, and f1-score on test data --x--x-\n",
      "\n",
      "Testing Accuracy:0.8982\n",
      "Testing f1_Score:0.8973\n",
      "Testing Precision:0.9021\n",
      "Testing recall_score:0.8925\n",
      "\n",
      "-x--x- Accuracy, Precision, Recall, and f1-score on train data -x--x-\n",
      "\n",
      "Training Accuracy:0.9136\n",
      "Training f1_Score:0.9133\n",
      "Training Precision:0.9174\n",
      "Training recall_score:0.9092\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "\n",
    "# instantiate the model \n",
    "logreg = LogisticRegression(max_iter = 5000)\n",
    "\n",
    "# fit the model with data\n",
    "logreg.fit(Xtrain,y_train)\n",
    "\n",
    "#Apply the trained Logistic Regression on Xtrain_std data to make predictions for the test data\n",
    "y_pred_test=logreg.predict(Xtest)\n",
    "\n",
    "#We measure the performance using the \"accuracy_score,f1_Score,Precision_score and recall_score\"\n",
    "print(\"\\n-x--x- Accuracy, Precision, Recall, and f1-score on test data --x--x-\\n\")\n",
    "\n",
    "print(f'Testing Accuracy:{accuracy_score(y_test, y_pred_test):.4f}')\n",
    "print(f'Testing f1_Score:{f1_score(y_test, y_pred_test):.4f}')\n",
    "print(f'Testing Precision:{precision_score(y_test, y_pred_test):.4f}')\n",
    "print(f'Testing recall_score:{recall_score(y_test, y_pred_test):.4f}')\n",
    "\n",
    "#Apply the trained perceptron on the data to make predicts for the trained data\n",
    "y_pred_tarin = logreg.predict(Xtrain)\n",
    "\n",
    "#We measure the performance using the \"accuracy_score\"\n",
    "\n",
    "print(\"\\n-x--x- Accuracy, Precision, Recall, and f1-score on train data -x--x-\\n\")\n",
    "\n",
    "print(f'Training Accuracy:{accuracy_score(y_train, y_pred_tarin):.4f}')\n",
    "print(f'Training f1_Score:{f1_score(y_train, y_pred_tarin):.4f}')\n",
    "print(f'Training Precision:{precision_score(y_train, y_pred_tarin):.4f}')\n",
    "print(f'Training recall_score:{recall_score(y_train, y_pred_tarin):.4f}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " -x--x- Accuracy, Precision, Recall, and f1-score on test data -x--x- \n",
      "\n",
      "Testing Accuracy:0.8695\n",
      "Testing f1_Score:0.8681\n",
      "Testing Precision:0.8740\n",
      "Testing recall_score:0.8622\n",
      "\n",
      " -x--x- Accuracy, Precision, Recall, and f1-score on train data -x--x- \n",
      "\n",
      "Training Accuracy:0.8866\n",
      "Training f1_Score:0.8864\n",
      "Training Precision:0.8891\n",
      "Training recall_score:0.8836\n"
     ]
    }
   ],
   "source": [
    "#Import Gaussian Naive Bayes model\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "#Create a Gaussian Classifier\n",
    "model = MultinomialNB()\n",
    "\n",
    "# Train the model using the training sets\n",
    "model.fit(Xtrain, y_train)\n",
    "\n",
    "#Predict Output\n",
    "y_pred_test=model.predict(Xtest)\n",
    "\n",
    "#We measure the performance using the \"accuracy_score,f1_Score,Precision_score and recall_score\"\n",
    "print(\"\\n -x--x- Accuracy, Precision, Recall, and f1-score on test data -x--x- \\n\")\n",
    "\n",
    "print(f'Testing Accuracy:{accuracy_score(y_test, y_pred_test):.4f}')\n",
    "print(f'Testing f1_Score:{f1_score(y_test, y_pred_test):.4f}')\n",
    "print(f'Testing Precision:{precision_score(y_test, y_pred_test):.4f}')\n",
    "print(f'Testing recall_score:{recall_score(y_test, y_pred_test):.4f}')\n",
    "\n",
    "#Apply the trained perceptron on the data to make predicts for the trained data\n",
    "y_pred_tarin = model.predict(Xtrain)\n",
    "\n",
    "#We measure the performance using the \"accuracy_score\"\n",
    "\n",
    "print(\"\\n -x--x- Accuracy, Precision, Recall, and f1-score on train data -x--x- \\n\")\n",
    "\n",
    "print(f'Training Accuracy:{accuracy_score(y_train, y_pred_tarin):.4f}')\n",
    "print(f'Training f1_Score:{f1_score(y_train, y_pred_tarin):.4f}')\n",
    "print(f'Training Precision:{precision_score(y_train, y_pred_tarin):.4f}')\n",
    "print(f'Training recall_score:{recall_score(y_train, y_pred_tarin):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleaning data\n",
    "\n",
    "def clean_text(\n",
    "    string: str, \n",
    "    punctuations=r'''!()-[]{};:'\"\\,<>./?@#$%^&*_~''',\n",
    "    stop_words=['the', 'a', 'and', 'is', 'be', 'will']) -> str:\n",
    "    \"\"\"\n",
    "    A method to clean text \n",
    "    \"\"\"\n",
    "    # Cleaning the urls\n",
    "    string = re.sub(r'https?://\\S+|www\\.\\S+', '', string)\n",
    "\n",
    "    # Cleaning the html elements\n",
    "    string = re.sub(r'<.*?>', '', string)\n",
    "\n",
    "    # Removing the punctuations\n",
    "    for x in string.lower(): \n",
    "        if x in punctuations: \n",
    "            string = string.replace(x, \"\") \n",
    "\n",
    "    # Converting the text to lower\n",
    "    string = string.lower()\n",
    "\n",
    "    # Removing stop words\n",
    "    string = ' '.join([word for word in string.split() if word not in stop_words])\n",
    "\n",
    "    # Cleaning the whitespaces\n",
    "    string = re.sub(r'\\s+', ' ', string).strip()\n",
    "\n",
    "    return string        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
